{
  "copyright_text": "Creative Commons Attribution license (reuse allowed)",
  "description": "Description\nWith the trend towards data streams, building successful streaming analysis systems means building a community comfortable with streaming tech. But getting started with stream processing can be intimidating for anyone. In this talk, I\u2019ll talk about designing and deploying a mini-testbed system to scale down the stream and how you can practice your favorite algorithm on an astronomical data stream.\n\nAbstract\nThe increasing availability of real-time data sources and the Internet of Things movement have pushed data analysis pipelines towards stream processing. But what does this really mean for my applications, and how do I have to change my code and workflow? In a new era of \u201cKappa architecture,\u201d it\u2019s easier than ever to use the same programming model for both batch and stream processing.\n\nFor those interested in the design and operations side, I will cover high-level design considerations for architecting a modular and scalable stream processing infrastructure that can support the flexibility of different use cases and can welcome a community of users who are more familiar with batch processing.\n\nFor the fast-batching Pythonistas, I\u2019ll talk about some of the advantages of using streaming tech in a data processing pipeline and how to make your life easier with 1) built-in replication, scalability, and stream \u201crewind\u201d for data distribution with Kafka, 2) structured messages with strictly enforced schemas and dynamic typing for fast parsing with Avro, and 3) a stream processing interface that is similar to batch with Spark that you can even use in a Jupyter notebook.\n\nWhen you\u2019re ready to jump into the stream, or at least take a drink from the fountain, I\u2019ll point you to an open source, containerized (with Docker), streaming ecosystem testbed that you can deploy to mock a stream of data and take your streaming analytics on a dry run over an astronomical data stream.\n\nwww.pydata.org\n\nPyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.\n\nPyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.",
  "duration": 2552,
  "language": "eng",
  "recorded": "2017-07-24",
  "related_urls": [
    {
      "label": "schedule",
      "url": "https://pydata.org/seattle2017/schedule"
    }
  ],
  "speakers": [
    "Maria Patterson"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/Co8XiL6242I/maxresdefault.jpg",
  "title": "Building a community fountain around your data stream",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Co8XiL6242I"
    }
  ]
}
