{
  "id": 256, 
  "category": "PyCon US 2010", 
  "slug": "pycon-2010--scrape-the-web--strategies-for-progra", 
  "title": "Scrape the Web: Strategies for programming websites that don't expect it", 
  "summary": "We'll discuss the basics of web scraping, and then dive into the details of\ndifferent methods and where they are most applicable. You'll leave with an\nunderstanding of when to apply different tools, and learn about automating a\nfull web browser, a \"heavy hammer\" that I picked up at a project for the\nElectronic Frontier Foundation.\n\n", 
  "description": "Scrape the Web: Strategies for programming websites that don't expect it\n\n  \nPresented by Asheesh Laroia\n\n  \nDo you find yourself faced with websites that have data you need to extract?\nWould your life be simpler if you could programmatically input data into web\napplications, even those tuned to resist interaction by bots?\n\n  \nYear by year, the web is becoming a stronger force. Learn how to get the best\nof it.\n\n  \nWe'll discuss the basics of web scraping, and then dive into the details of\ndifferent methods and where they are most applicable. You'll leave with an\nunderstanding of when to apply different tools, and learn about automating a\nfull web browser, a \"heavy hammer\" that I picked up at a project for the\nElectronic Frontier Foundation.\n\n  \nAtendees should bring a laptop, if possible, to try the examples we discuss\nand optionally take notes. Code samples will be made available after class\nwith no restrictions. Intended Audience\n\n  \nIntermediate (or better) Python programmers, probably without extensive web\ntesting experience\n\n  \nClass Outline\n\n  * My motto: \"The website is the API.\" \n  * Choosing a parser: BeautifulSoup, lxml, HTMLParse, and html5lib. \n  * Extracting information, even in the face of bad HTML: Regular expressions, BeautifulSoup, SAX, and XPath. \n  * Automatic template reverse-engineering tools. \n  * Submitting to forms. \n  * Playing with XML-RPC \n  * DO NOT BECOME AN EVIL COMMENT SPAMMER. \n  * Countermeasures, and circumventing them: \n    * IP address limits \n    * Hidden form fields \n    * User-agent detection \n    * JavaScript \n    * CAPTCHAs \n  * Plenty of full source code to working examples: \n    * Submitting to forms for text-to-speech. \n    * Downloading music from web stores. \n    * Automating Firefox with Selenium RC to navigate a pure-JavaScript service. \n  * Q&A; and workshopping \n  * Use your power for good, not evil. \n\n", 
  "quality_notes": "", 
  "language": "English", 
  "copyright_text": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0", 
  "thumbnail_url": "http://a.images.blip.tv/Pycon-PyCon2010ScrapeTheWebStrategiesForProgrammingWebsitesTha613-141.jpg", 
  "duration": null, 
  "videos": [
    {
      "url": "http://05d2db1380b6504cc981-8cbed8cf7e3a131cd8f1c3e383d10041.r93.cf2.rackcdn.com/pycon-us-2010/256_scrape-the-web-strategies-for-programming-websites-that-don-t-expect-it.m4v", 
      "length": null, 
      "type": "mp4"
    }, 
    {
      "url": "http://05d2db1380b6504cc981-8cbed8cf7e3a131cd8f1c3e383d10041.r93.cf2.rackcdn.com/pycon-us-2010/256_scrape-the-web-strategies-for-programming-websites-that-don-t-expect-it.ogv", 
      "length": 851529869, 
      "type": "ogv"
    }
  ], 
  "source_url": "", 
  "tags": [
    "pycon", 
    "pycon2010", 
    "scraping", 
    "web"
  ], 
  "speakers": [
    "Asheesh Laroia"
  ], 
  "recorded": "2010-02-19"
}