{"whiteboard": "", "video_mp4_download_only": true, "video_webm_download_only": false, "duration": 30, "video_ogv_download_only": false, "category": "PyCon AU 2013", "speakers": ["Roger Barnes"], "title": "Building data flows with Celery and SQLAlchemy", "quality_notes": "", "video_flv_length": null, "recorded": "2013-07-07", "video_mp4_length": null, "description": "", "video_mp4_url": "http://s3.us.archive.org/ndvpyconau2013/Building_data_flows_with_Celer.mp4", "tags": [], "copyright_text": "CC-BY-SA", "related_urls": [], "video_flv_download_only": false, "source_url": "https://www.youtube.com/watch?v=AhIoAMltzVw", "video_webm_url": null, "video_ogv_length": null, "video_ogv_url": null, "language": "English", "video_webm_length": null, "summary": "Reporting and analysis systems rely on coherent and reliable data, often from disparate sources. To that end, a series of well established data warehousing practices have emerged to extract data and produce a consistent data store.\r\n\r\nThis talk will look at some options for composing workflows using Python. In particular, we'll explore beyond Celery's asynchronous task processing functionality into its workflow (aka Canvas) system and how it can be used in conjunction with SQLAlchemy's architecture to provide the building blocks for data stream processing.\r\n", "thumbnail_url": "http://i1.ytimg.com/vi/AhIoAMltzVw/hqdefault.jpg", "video_flv_url": null}